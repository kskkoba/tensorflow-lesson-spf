{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1lZG0PJPWqEExVVatfx1vhslmAs_p54R6","timestamp":1590552641335},{"file_id":"https://github.com/lmoroney/mlday-tokyo/blob/master/Lab4-Using-Convolutions.ipynb","timestamp":1589585325495}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"R6gHiH-I7uFa"},"source":["#Improving Computer Vision Accuracy using Convolutions\n","\n","\n","前回のレッスンでは、入力層（データの形をした）、出力層（希望する出力の形をした）、そして隠れ層の3つの層を含むディープニューラルネットワーク（DNN）を使ってファッション認識を行う方法を見ました。隠蔽層の大きさ、訓練エポック数などが最終的な精度に与える影響を実験しました。\n","\n","便宜上、ここにコード全体をもう一度示します。これを実行して、最後にプリントアウトされたテスト精度をメモしておいてください。"]},{"cell_type":"code","metadata":{"id":"xcsRtq9OLorS"},"source":["import tensorflow as tf\n","mnist = tf.keras.datasets.fashion_mnist\n","(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n","training_images=training_images / 255.0\n","test_images=test_images / 255.0\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n","  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n","])\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(training_images, training_labels, epochs=5)\n","\n","test_loss = model.evaluate(test_images, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zldEXSsF8Noz"},"source":["あなたの精度はおそらくトレーニングで約89%、検証で約87%です...悪くないですね...でも、どうやってそれをさらに良くするのでしょうか？一つの方法は畳み込み（Convolutions）と呼ばれるものを使うことです。ここでは畳み込みの詳細については触れませんが、究極のコンセプトは、画像の内容を絞り込んで、特定の明確な細部に焦点を当てるというものです。\n","\n","フィルターを使った画像処理をしたことがある人ならば (https://en.wikipedia.org/wiki/Kernel_(image_processing) のように)、 畳み込みは非常に見慣れたものになるでしょう。\n","\n","要するに、配列 (通常は 3x3 や 5x5) を受け取り、それを画像に渡します。その行列の中の式に基づいて基礎となるピクセルを変更することで、エッジ検出のようなことができます。例えば、上のリンクを見てみると、3x3はエッジ検出用に定義されており、真ん中のセルが8で、隣り合うセルはすべて-1となっています。 この場合、各ピクセルの値に8をかけ、隣り合うセルの値を差し引きます。これを各画素ごとに行うと、エッジが強調された新しい画像ができあがります。\n","\n","![元画像](https://upload.wikimedia.org/wikipedia/commons/5/50/Vd-Orig.png)\n","![変換後](https://upload.wikimedia.org/wikipedia/commons/6/6d/Vd-Edge3.png)\n","\n","これはコンピュータビジョンに最適です。このようにハイライトされた特徴によって、あるアイテムを別のアイテムと区別することができ、必要とされる情報量はずっと少なくて済みます。なぜならハイライトされた部分だけ学習させればいいわけですので。\n","\n","これが畳み込みニューラルネットワークのコンセプトです。全結合層の前にいくつかの畳み込み層を追加すると、全結合層に送られる情報はより集中され、より正確になる可能性があります。\n","\n","以下のコードを実行してみてください -- これは先ほどと同じニューラルネットワークですが、今回は畳み込み層を最初に追加しています。時間はかかりますが、精度への影響を見てください。"]},{"cell_type":"code","metadata":{"id":"C0tFgT1MMKi6"},"source":["import tensorflow as tf\n","print(tf.__version__)\n","mnist = tf.keras.datasets.fashion_mnist\n","(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n","training_images=training_images.reshape(60000, 28, 28, 1)\n","training_images=training_images / 255.0\n","test_images = test_images.reshape(10000, 28, 28, 1)\n","test_images=test_images/255.0\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","  tf.keras.layers.MaxPooling2D(2,2),\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","model.fit(training_images, training_labels, epochs=5)\n","test_loss = model.evaluate(test_images, test_labels)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uRLfZ0jt-fQI"},"source":["トレーニングデータでは93％、検証データでは91％くらいまで上がっているのではないでしょうか。\n","\n","これは重要な一歩です。\n","\n","より多くのエポックで実行してみてください - 例えば、約20のエポックで、結果をみてください。結果はとても良いように見えるかもしれませんが、後述する「過学習」と呼ばれるものが原因で、検証結果は実際には下がっているかもしれません。\n","\n","(簡単に言えば、「過学習」とは、ネットワークが訓練セットからのデータをよく学習しているにもかかわらず、そのデータだけに特化しすぎていて、結果的に違う（見たことのない）データを判断することができなくなることです。例えば、人生の中で赤い靴しか見たことがなかった場合、赤い靴を見たときにそれを識別するのは非常に得意ですが、青いスエードの靴を見たときには混乱するかもしれません。)\n","\n","それから、もう一度コードを見て、畳み込みがどのように構築されたかを一歩一歩見てください。"]},{"cell_type":"markdown","metadata":{"id":"RaLX5cgI_JDb"},"source":["ステップ1はデータを収集することです。ここで、学習データの形を変える必要があるという点で、少し変化があることに気づくでしょう。これは、最初の畳み込みではすべてを含む1つのテンソルを想定しているからです。そのため、60,000個の28x28x1のリストの代わりに、60,000x28x28x1の1つの4次元リストを作成し、テスト画像についても同様です。このようにしないと、畳み込み層が形状を認識しないため、トレーニング時にエラーが出てしまいます。\n","\n","\n","```\n","import tensorflow as tf\n","mnist = tf.keras.datasets.fashion_mnist\n","(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n","training_images=training_images.reshape(60000, 28, 28, 1)\n","training_images=training_images / 255.0\n","test_images = test_images.reshape(10000, 28, 28, 1)\n","test_images=test_images/255.0\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SS_W_INc_kJQ"},"source":["次にモデルを定義します。ここでは、一番上の入力レイヤーの代わりに Convolution を追加します。パラメータは以下の通りです。\n","\n","1. 生成するフィルター（カーネル）の数。純粋に任意ですが、32 くらいから始めるのが良いでしょう。\n","2. フィルターのサイズ（この場合は3x3）。\n","3. 使用する活性化関数 -- ここでは relu を使用します。これは x>0 のときに x を返し、そうでなければ 0 を返すことと同等のものです。\n","4. 最初のレイヤーで利用する畳み込みは、入力データの形状を指定します。\n","\n","コンボリューションで強調された特徴の内容を維持しつつ、画像を圧縮するように設計されたMaxPoolingレイヤーでコンボリューションを追いかけます。MaxPooling に (2,2) を指定すると、画像のサイズが 4 分の 1 になります。ここではあまり詳しく説明しませんが、ピクセルの2x2配列を作成し、一番大きいものを選び、4ピクセルを1ピクセルに変換します。 これを画像全体で繰り返すことで、水平方向のピクセル数を半分に、垂直方向のピクセル数を半分にし画像を25%に縮小します。\n","\n","ネットワークの大きさや形を見るには model.summary() を呼び出すことができ、MaxPooling レイヤーを重ねるごとに画像サイズがこのように縮小されていることがわかると思います。\n","\n","```\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RMorM6daADjA"},"source":["さらに一つの畳込み（Convolution）を追加します。\n","```\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","  tf.keras.layers.MaxPooling2D(2,2)\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"b1-x-kZF4_tC"},"source":["出力を平坦化します。これ以降は畳み込みを利用していないDNNと同じ構造になります。\n","\n","```\n","  tf.keras.layers.Flatten(),\n","```"]},{"cell_type":"markdown","metadata":{"id":"qPtqR23uASjX"},"source":["全結合層を１２８層、出力層を１０層としたのは、畳み込みを利用しない場合と同じです。\n","\n","```\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","```"]},{"cell_type":"markdown","metadata":{"id":"C0GSsjUhAaSj"},"source":["ここでモデルをコンパイルし、fitメソッドを呼び出して学習を行ったあと、テストセットを利用して損失と精度を評価します。\n","\n","\n","\n","```\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(training_images, training_labels, epochs=5)\n","test_loss, test_acc = model.evaluate(test_images, test_labels)\n","print(test_acc)\n"]},{"cell_type":"markdown","metadata":{"id":"IXx_LX3SAlFs"},"source":["# Visualizing the Convolutions and Pooling\n","このコードは、畳み込みの状況をグラフィカルに表示してくれます。print (test_labels[:100])は、テストセットの最初の100個のラベルを示しています。インデックス0、インデックス23、インデックス28のラベルはすべて同じ値(9)であることがわかります。これらはすべて靴です。それぞれに畳み込みを実行した結果を見てみましょう。畳み込み後のDNNがそのデータで学習しているときには、より少ない量のデータで学習しているので、この畳み込みとプールの組み合わせの出力から靴の間の共通性を見つけているのかもしれません。"]},{"cell_type":"code","metadata":{"id":"f-6nX4QsOku6"},"source":["print(test_labels[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9FGsHhv6JvDx"},"source":["import matplotlib.pyplot as plt\n","f, axarr = plt.subplots(3,4)\n","FIRST_IMAGE=0\n","SECOND_IMAGE=23\n","THIRD_IMAGE=28\n","CONVOLUTION_NUMBER = 1\n","from tensorflow.keras import models\n","layer_outputs = [layer.output for layer in model.layers]\n","activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n","for x in range(0,4):\n","  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n","  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n","  axarr[0,x].grid(False)\n","  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n","  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n","  axarr[1,x].grid(False)\n","  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n","  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n","  axarr[2,x].grid(False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8KVPZqgHo5Ux"},"source":["EXERCISES\n","\n","1. 畳み込みを編集してみてください。32を16または64に変更してください。これは精度やトレーニング時間にどのような影響を与えるでしょうか。\n","\n","2. 最後の畳み込みを削除します．これは精度やトレーニング時間にどのような影響を与えますか?\n","\n","3. Convolution を追加してみてはどうでしょうか? これはどのような影響があると思いますか？実験してみてください．\n","\n","4. 最初のConvolutions以外のすべてのConvolutionsを削除します。これはどのような影響があると思いますか？実験してみてください。\n","\n","5. 前回のレッスンでは、損失関数をチェックして、一定の量に達したら学習をキャンセルするコールバックを実装しました。ここでそれを実装できるかどうか見てみてください。"]},{"cell_type":"code","metadata":{"id":"ZpYRidBXpBPM"},"source":["import tensorflow as tf\n","print(tf.__version__)\n","mnist = tf.keras.datasets.mnist\n","(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n","training_images=training_images.reshape(60000, 28, 28, 1)\n","training_images=training_images / 255.0\n","test_images = test_images.reshape(10000, 28, 28, 1)\n","test_images=test_images/255.0\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(training_images, training_labels, epochs=10)\n","test_loss, test_acc = model.evaluate(test_images, test_labels)\n","print(test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bp26CBrQqPUR"},"execution_count":null,"outputs":[]}]}